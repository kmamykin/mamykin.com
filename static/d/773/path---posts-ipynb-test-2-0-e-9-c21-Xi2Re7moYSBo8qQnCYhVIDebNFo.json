{"data":{"site":{"siteMetadata":{"site":"https://mamykin.com","disqusShortname":"mamykin-com"}},"post":{"frontmatter":{"permalink":"/posts/ipynb-test2/","title":null,"author":null,"date":null,"image":null,"tags":[""]},"content":{"type":"Notebook","markdown":null,"notebook":"{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<!-- dom:TITLE: A practical introduction to sensitivity analysis -->\\n\",\n    \"# A practical introduction to sensitivity analysis\\n\",\n    \"<!-- dom:AUTHOR: Leif Rune Hellevik at Department of Structural Engineering, NTNU -->\\n\",\n    \"<!-- Author: -->  \\n\",\n    \"**Leif Rune Hellevik**, Department of Structural Engineering, NTNU\\n\",\n    \"\\n\",\n    \"Date: **Jul 13, 2018**\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 1,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# ipython magic\\n\",\n    \"%matplotlib notebook\\n\",\n    \"%load_ext autoreload\\n\",\n    \"%autoreload 2\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 2,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# plot configuration\\n\",\n    \"import matplotlib\\n\",\n    \"import matplotlib.pyplot as plt\\n\",\n    \"plt.style.use(\\\"ggplot\\\")\\n\",\n    \"# import seaborn as sns # sets another style\\n\",\n    \"matplotlib.rcParams['lines.linewidth'] = 3\\n\",\n    \"fig_width, fig_height = (7.0,5.0)\\n\",\n    \"matplotlib.rcParams['figure.figsize'] = (fig_width, fig_height)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 3,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# import modules\\n\",\n    \"import numpy as np\\n\",\n    \"from numpy import linalg as LA\\n\",\n    \"import chaospy as cp\\n\",\n    \"from sensitivity_examples_nonlinear import generate_distributions\\n\",\n    \"from monte_carlo import generate_sample_matrices_mc\\n\",\n    \"from monte_carlo import calculate_sensitivity_indices_mc\\n\",\n    \"import pandas as pd\\n\",\n    \"from operator import index\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Introduction\\n\",\n    \"<div id=\\\"sec:introduction\\\"></div>\\n\",\n    \"\\n\",\n    \"This practical introduction to sensitivity analysis is based on the\\n\",\n    \"presentation and examples found in [[saltelli_global_2008]](#saltelli_global_2008). To give\\n\",\n    \"the reader an even better hands on experience of the topic, we have\\n\",\n    \"integrated the computations in a python notebook format.\\n\",\n    \"\\n\",\n    \"Many sensitivity analyses reported in the literature are based on\\n\",\n    \"derivatives at set point or point of interest. Indeed such approaches\\n\",\n    \"are based on the fact that the derivative of $\\\\partial Y_i/\\\\partial\\n\",\n    \"X_j$ of quantity of interest $Y_i$ as a function of an input variable\\n\",\n    \"$X_j$ can be thought of as the mathematical definition of the\\n\",\n    \"sensitivity of $Y_i$ versus $X_j$.\\n\",\n    \"\\n\",\n    \"However, what is important to keep in mind is that local derivatives\\n\",\n    \"are only informative at the set point in the parameter space at which\\n\",\n    \"they are computed, and do not provide information for the rest of the\\n\",\n    \"parameter space. Naturally, such a linearisation will matter little\\n\",\n    \"for linear models, but for general, nonlinear models, care must be\\n\",\n    \"taken.  In particular this is important in situations when the input\\n\",\n    \"parameters are uncertain.\\n\",\n    \"\\n\",\n    \"# Local versus global sensitivity analysis\\n\",\n    \"\\n\",\n    \"Motivation and useful purposes of sensitivity analysis\\n\",\n    \"\\n\",\n    \" * Parameter prioritization of parameters of high sensitivity (importance)\\n\",\n    \"\\n\",\n    \" * Parameter fixation of parameters of low sensitivity (importance)\\n\",\n    \"\\n\",\n    \" * Reveal surprising relations/properties of the model\\n\",\n    \"\\n\",\n    \" * Indentify critical regions in the input parameter space\\n\",\n    \"\\n\",\n    \"## Local approaches based on derivatives\\n\",\n    \"\\n\",\n    \"Many sensitivity analyses found in the scientific literature are based\\n\",\n    \"on derivatives.  This fact has naturally a rational basis as the\\n\",\n    \"partial derivative $\\\\partial y/\\\\partial Z_i$ of a model predicion $y$\\n\",\n    \"with respect to an input $Z_i$, can be understood as the mathematical\\n\",\n    \"representation of the sensitivity of $y$ with respect to $Z_i$.\\n\",\n    \"\\n\",\n    \"Even though a local, partial derivative approach is computationallyXS\\n\",\n    \"inexpensive it has in general limited usage for nonlinear models. The\\n\",\n    \"derivatives are linearizations of the model sensitivities around the\\n\",\n    \"point in the parameter space at which they are evaluated, and may only\\n\",\n    \"be extrapolated to provide information on the sensitivity in other\\n\",\n    \"regions of the parameter space in the case of a linear model.\\n\",\n    \"\\n\",\n    \"As a first motivation you may consider the ratio areas of circle with\\n\",\n    \"diameter $D$ to the corresponding square with sides $D$, as an optimistic approximation of the portion of the parameter space you might represent with a one factor at the time (OAT) approach:\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<!-- Equation labels as ordinary links -->\\n\",\n    \"<div id=\\\"_auto1\\\"></div>\\n\",\n    \"\\n\",\n    \"$$\\n\",\n    \"\\\\begin{equation}\\n\",\n    \"\\\\frac{1}{4} \\\\; \\\\frac{\\\\pi \\\\, D^2}{D^2} \\\\approx 3/4 \\n\",\n    \"\\\\label{_auto1} \\\\tag{1}\\n\",\n    \"\\\\end{equation}\\n\",\n    \"$$\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<!-- dom:FIGURE: [figs/hypersphere.png, width=400 frac=0.7] Area ratio of circle to a square. -->\\n\",\n    \"<!-- begin figure -->\\n\",\n    \"\\n\",\n    \"<p>Area ratio of circle to a square.</p>\\n\",\n    \"<img src=\\\"figs/hypersphere.png\\\" width=400>\\n\",\n    \"\\n\",\n    \"<!-- end figure -->\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"You may compute the corresonding ratio for a sphere and a cube\\n\",\n    \"yourself. For higher dimensions we provide a code snippet below which\\n\",\n    \"calculates the ratio of a [hypersphere](https://en.wikipedia.org/wiki/N-sphere#Recurrences) to a\\n\",\n    \"hypercube. This is you motivate why you should avoid \\\"one factor at\\n\",\n    \"the time\\\" analyses for sensitivity analysis in higher dimensions with multiple parameters.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 4,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# See https://en.wikipedia.org/wiki/N-sphere#Recurrences\\n\",\n    \"\\n\",\n    \"%matplotlib nbagg\\n\",\n    \"import ipywidgets as widgets\\n\",\n    \"import matplotlib.pyplot as plt\\n\",\n    \"import interactive_pie\\n\",\n    \"from interactive_pie import update_pie\\n\",\n    \"\\n\",\n    \"plt.suptitle('Why you should avoid \\\"one factor at the time\\\" in higher dimensions.')\\n\",\n    \"w_dim = widgets.IntSlider(min=1, max=20, value=2, description='Dimensions')\\n\",\n    \"widgets.interactive(update_pie, Ndim=w_dim)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Based on the brief motivation above we will present of methods based\\n\",\n    \"on the exploration of the input parameter space by judiciously\\n\",\n    \"selecting samples in that space. Such approaches result in more robust and\\n\",\n    \"informative sensitivity measures, than what would be the result from a local\\n\",\n    \"derivative app\\t\\troach at the center of the parameter space.\\n\",\n    \"\\n\",\n    \"To introduce the methods of sensitivity analysis, we shall\\n\",\n    \"start from derivatives and illustrate them on a very simple linear\\n\",\n    \"model.\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"# A simple linear model\\n\",\n    \"\\n\",\n    \"As an simple linear model example consider:\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<!-- Equation labels as ordinary links -->\\n\",\n    \"<div id=\\\"eq:linear_model\\\"></div>\\n\",\n    \"\\n\",\n    \"$$\\n\",\n    \"\\\\begin{equation}\\n\",\n    \"Y = \\\\sum_{i=1}^{r} \\\\Omega_i \\\\, Z_i\\n\",\n    \"\\\\label{eq:linear_model} \\\\tag{2}\\n\",\n    \"\\\\end{equation}\\n\",\n    \"$$\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"where the input factors are $\\\\mathbf{X} = (\\\\Omega_1, \\\\Omega_2, \\\\ldots,\\n\",\n    \"\\\\Omega_r, Z_1, Z_2, \\\\ldots, Z_r)$. For simplicity we assume that the\\n\",\n    \"model output $Y$ of ([2](#eq:linear_model)) is a single variable and\\n\",\n    \"that the $\\\\Omega s$ are fixed coefficients or weights.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<!-- Equation labels as ordinary links -->\\n\",\n    \"<div id=\\\"_auto2\\\"></div>\\n\",\n    \"\\n\",\n    \"$$\\n\",\n    \"\\\\begin{equation}\\n\",\n    \" \\\\Omega_1=\\\\Omega_2=\\\\ldots=\\\\text{constant}\\n\",\n    \"\\\\label{_auto2} \\\\tag{3}\\n\",\n    \"\\\\end{equation}\\n\",\n    \"$$\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Consequently, the true factors of ([2](#eq:linear_model)) are just\\n\",\n    \"$(Z_1, Z_2, \\\\ldots, Z_r)$. The individual variables\\n\",\n    \"$Z_i$ are taken to normally distributed with mean zero\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<!-- Equation labels as ordinary links -->\\n\",\n    \"<div id=\\\"eq:NZi\\\"></div>\\n\",\n    \"\\n\",\n    \"$$\\n\",\n    \"\\\\begin{equation} Z_i \\\\sim N(0, \\\\sigma_{Z_i}), \\\\qquad i=1,2, \\\\ldots, r\\n\",\n    \"\\\\label{eq:NZi} \\\\tag{4} \\\\end{equation}\\n\",\n    \"$$\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"As the predicted value $Y$ of the model in ([2](#eq:linear_model)) is\\n\",\n    \"linear combination of normally distributed factors, it is easy to\\n\",\n    \"verify (see exercices in [[saltelli_global_2008]](#saltelli_global_2008)) that $Y$ also will\\n\",\n    \"be normally distributed with:\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<!-- Equation labels as ordinary links -->\\n\",\n    \"<div id=\\\"eq:analytic_mean_std\\\"></div>\\n\",\n    \"\\n\",\n    \"$$\\n\",\n    \"\\\\begin{equation}\\n\",\n    \"\\\\bar{y} = \\\\sum_{i=1}^{r} \\\\Omega_i \\\\; \\\\bar{z}_i, \\\\qquad \\\\sigma_Y = \\\\sqrt{\\\\sum_{i=1}^{r} \\\\Omega_i^2 \\\\, \\\\sigma_{Z_i}^2}\\n\",\n    \"\\\\label{eq:analytic_mean_std} \\\\tag{5}\\t\\n\",\n    \"\\\\end{equation}\\n\",\n    \"$$\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Furthermore, we order the factors from the most certain to the less\\n\",\n    \"certain, i.e.:\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<!-- Equation labels as ordinary links -->\\n\",\n    \"<div id=\\\"_auto3\\\"></div>\\n\",\n    \"\\n\",\n    \"$$\\n\",\n    \"\\\\begin{equation}\\n\",\n    \" \\\\sigma_{Z_1} <  \\\\sigma_{Z_2} <  \\\\ldots  <  \\\\sigma_{Z_r}\\n\",\n    \"\\\\label{_auto3} \\\\tag{6}\\n\",\n    \"\\\\end{equation}\\n\",\n    \"$$\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Scatterplots versus derivatives\\n\",\n    \"\\n\",\n    \"We have implemented the simple linear model in ([2](#eq:linear_model)) in python as:\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 5,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# start the linear model\\n\",\n    \"def linear_model(w, z):\\n\",\n    \"    return np.sum(w*z, axis=1)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"To hold the mean and the standard deviation of all the input factors\\n\",\n    \"we use a numpy-array of size $r\\\\times 2$, with one row per factor,\\n\",\n    \"where the first column holds the mean whereas the second column holds\\n\",\n    \"the standard deviation. The weights $\\\\Omega_{1\\\\ldots r}$ are stored in a numpy-vector.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 6,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"    # Set mean (column 0) and standard deviations (column 1) for each factor z. Nrv=nr. rows\\n\",\n    \"    Nrv = 4  # number of random variables \\n\",\n    \"    zm = np.array([[0., i] for i in range(1, Nrv + 1)])\\n\",\n    \"\\n\",\n    \"    # The above \\\"list comprehension\\\" is equivalent to the next for lines\\n\",\n    \"    # zm = np.zeros((Nrv, 2))\\n\",\n    \"    # zm[0, 1] = 1\\n\",\n    \"    # zm[1, 1] = 2\\n\",\n    \"    # zm[2, 1] = 3\\n\",\n    \"    # zm[3, 1] = 4\\n\",\n    \"\\n\",\n    \"    # Set the weight\\n\",\n    \"    c = 2\\n\",\n    \"    w = np.ones(Nrv) * c\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"We may now perform a Monte Carlo experiment on our model by generating $N$ samples from the distributions of each factor and an input sample is thus produced:\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<!-- Equation labels as ordinary links -->\\n\",\n    \"<div id=\\\"eq:mc_sample\\\"></div>\\n\",\n    \"\\n\",\n    \"$$\\n\",\n    \"\\\\begin{equation}\\n\",\n    \"\\\\mathbf{Z} = \\\\left [\\n\",\n    \"\\\\begin{array}{cccc}\\n\",\n    \"Z_{1,1} & Z_{1,2}  & \\\\ldots & Z_{1,N} \\\\\\\\ \\n\",\n    \"Z_{2,1} & Z_{2,2}  & \\\\ldots & Z_{2,N}\\\\\\\\ \\n\",\n    \"\\\\vdots & \\\\vdots & \\\\vdots & \\\\vdots \\\\\\\\ \\n\",\n    \"Z_{r,1} & Z_{r,2}  & \\\\ldots & Z_{r,N}\\n\",\n    \"\\\\end{array} \\n\",\n    \"\\\\right ]\\n\",\n    \"\\\\label{eq:mc_sample} \\\\tag{7}\\n\",\n    \"\\\\end{equation}\\n\",\n    \"$$\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"We may the compute a value of $Y$ from ([2](#eq:linear_model)) for each\\n\",\n    \"column in ([7](#eq:mc_sample)) to produce a solution vector\\n\",\n    \"$\\\\mathbf{Y}$. Having sampled $N$ values from each input factor we may\\n\",\n    \"produce $r$ scatter plots, by projecting in turn the $N$ values of\\n\",\n    \"$\\\\mathbf{Y}$ against the $N$ values of each of the $r$ input factors.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<!-- Equation labels as ordinary links -->\\n\",\n    \"<div id=\\\"eq:mc_solution\\\"></div>\\n\",\n    \"\\n\",\n    \"$$\\n\",\n    \"\\\\begin{equation}\\n\",\n    \"\\\\mathbf{Y} = \\\\left [\\n\",\n    \"\\\\begin{array}{c}\\n\",\n    \"y_1 \\\\\\\\ \\n\",\n    \"y_2 \\\\\\\\ \\n\",\n    \"\\\\vdots \\\\\\\\ \\n\",\n    \"y_N\\n\",\n    \"\\\\end{array}\\n\",\n    \"\\\\right ]\\n\",\n    \"\\\\label{eq:mc_solution} \\\\tag{8}\\n\",\n    \"\\\\end{equation}\\n\",\n    \"$$\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 7,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"    # Generate distributions for each element in z and sample\\n\",\n    \"    Ns = 500\\n\",\n    \"    # jpdf = generate_distributions(zm)\\n\",\n    \"    \\n\",\n    \"    pdfs = []\\n\",\n    \"\\n\",\n    \"    for i, z in enumerate(zm):\\n\",\n    \"        pdfs.append(cp.Normal(z[0], z[1]))\\n\",\n    \"\\n\",\n    \"    jpdf = cp.J(*pdfs)\\n\",\n    \"\\n\",\n    \"    # generate Z\\n\",\n    \"    Z = jpdf.sample(Ns)\\n\",\n    \"    # evaluate the model\\n\",\n    \"    Y = linear_model(w, Z.transpose())\\n\",\n    \"    print(np.var(Y))\\n\",\n    \"\\n\",\n    \"    # Scatter plots of data for visual inspection of sensitivity\\n\",\n    \"    fig=plt.figure()\\n\",\n    \"    for k in range(Nrv):\\n\",\n    \"        plt.subplot(2, 2, k + 1)\\n\",\n    \"        plt.plot(Z[k, :], Y[:], '.')\\n\",\n    \"        xlbl = 'Z' + str(k)\\n\",\n    \"        plt.xlabel(xlbl)\\n\",\n    \"        \\n\",\n    \"    fig.tight_layout()  # adjust subplot(s) to the figure area.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Note that the assumption of independent factors $Z_i$ allows us to sample\\n\",\n    \"each $Z_i$ independently from its own marginal distribution. We store\\n\",\n    \"all the samples for all the factors $Z_i$ in the the numpy array\\n\",\n    \"`Z[i,:]`, where $i$ corresponds to $Z_i$ as:\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"        pdf.append(cp.Normal(z[0],z[1]))\\n\",\n    \"            Z[i,:]=pdf[i].sample(N)\\n\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"From the scatterplots generated by the python code above we\\n\",\n    \"intuitively get the impression that $Y$ is more sensitive to $Z_4$\\n\",\n    \"than to $Z_3$, and that $Y$ is more sensitive to $Z_3$ than to $Z_3$,\\n\",\n    \"and that we may order the factors my influence on $Y$ as:\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<!-- Equation labels as ordinary links -->\\n\",\n    \"<div id=\\\"eq:scatter_plot_rank\\\"></div>\\n\",\n    \"\\n\",\n    \"$$\\n\",\n    \"\\\\begin{equation}\\n\",\n    \"Z_4 > Z_3 > Z_2 > Z_1 \\n\",\n    \"\\\\label{eq:scatter_plot_rank} \\\\tag{9}\\n\",\n    \"\\\\end{equation}\\n\",\n    \"$$\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Our intuitive notion of influence is based on that there is more shape\\n\",\n    \"(or better pattern) in the plot for $Z_4$ than for $Z_3$ and likewise.\\n\",\n    \"\\n\",\n    \"For our simple linear model in ([2](#eq:linear_model)) we are in the\\n\",\n    \"fortunate situation that we may compute the local derivatives analyticaly:\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<!-- Equation labels as ordinary links -->\\n\",\n    \"<div id=\\\"eq:Sp\\\"></div>\\n\",\n    \"\\n\",\n    \"$$\\n\",\n    \"\\\\begin{equation}\\n\",\n    \"S_{Z_i}^{p} = \\\\frac{\\\\partial Y}{\\\\partial Z_i} = \\\\Omega_i\\n\",\n    \"\\\\label{eq:Sp} \\\\tag{10}\\n\",\n    \"\\\\end{equation}\\n\",\n    \"$$\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"In our code example we set all the $\\\\Omega_i=2$ for $i=1,\\\\ldots,4$,\\n\",\n    \"and according to the local sensitivity meansure $S_{Z_i}^{p}$ in\\n\",\n    \"([10](#eq:Sp)) all the input factors $Z_i$s are equally important and\\n\",\n    \"independent of the variation of each factor. This measure is clearly\\n\",\n    \"at odds with the ranking of influence based on the scatterplots in\\n\",\n    \"([9](#eq:scatter_plot_rank)) and is an indication of the usefullness of\\n\",\n    \"scatterplots in sensititivy analysis. However, the bidimensional\\n\",\n    \"scatterplots may in some cases be deceiving and lead to type II\\n\",\n    \"errors (i.e. failure to identify influential parameters). ref to Saltelli 2004...\\n\",\n    \"\\n\",\n    \"Most sensitivity measures aim to preserve the rich information\\n\",\n    \"provided by the scatterplots in a condensed format. The challenge is\\n\",\n    \"how to rank the factors rapidly and automatically without having to\\n\",\n    \"inspect many scatterplots in situations with many input\\n\",\n    \"factors. Another challenge with scatterplots is that sensitivities for\\n\",\n    \"sets cannot be visualized, while luckily compact sensitivity measures may be\\n\",\n    \"defined in such cases.\\n\",\n    \"\\n\",\n    \"# Normalized derivatives\\n\",\n    \"\\n\",\n    \"A simple way to improve the derivative sensitivity measure $S_{Z_i}^{p}$ in\\n\",\n    \"([10](#eq:Sp)) is to scale the input-output variables with their standard deviations:\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<!-- Equation labels as ordinary links -->\\n\",\n    \"<div id=\\\"eq:Ss\\\"></div>\\n\",\n    \"\\n\",\n    \"$$\\n\",\n    \"\\\\begin{equation}\\n\",\n    \"S_{Z_i}^{\\\\sigma} = \\\\frac{\\\\partial Y/\\\\sigma_Y}{\\\\partial Z_i/\\\\sigma_{Z_i}} = \\\\frac{\\\\sigma_{Z_i}}{\\\\sigma_{Y}} \\\\; \\\\frac{\\\\partial Y}{\\\\partial Z_i}\\n\",\n    \"\\\\label{eq:Ss} \\\\tag{11}\\n\",\n    \"\\\\end{equation}\\n\",\n    \"$$\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"In case of our simple linear model ([2](#eq:linear_model)) we get from\\n\",\n    \"([11](#eq:Ss)):\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<!-- Equation labels as ordinary links -->\\n\",\n    \"<div id=\\\"eq:Ss_simple\\\"></div>\\n\",\n    \"\\n\",\n    \"$$\\n\",\n    \"\\\\begin{equation}\\n\",\n    \"\\\\left (S_{Z_i}^{\\\\sigma} \\\\right)^2 = \\\\left( \\\\frac{\\\\sigma_{Z_i}}{\\\\sigma_{Y}}\\\\right)^2 \\\\; \\\\left (\\\\frac{\\\\partial Y}{\\\\partial Z_i}\\\\right)^2 = \\\\left( \\\\frac{\\\\sigma_{Z_i}\\\\, \\\\Omega_i}{\\\\sigma_{Y}}\\\\right)^2 \\\\;  \\\\qquad \\\\textsf{which may be rearranged to:} \\\\qquad \\\\sigma_y^2 \\\\, (S_{Z_i}^{\\\\sigma})^2 = \\\\left ( \\\\Omega_{i} \\\\sigma_{Y} \\\\right )^2\\n\",\n    \"\\\\label{eq:Ss_simple} \\\\tag{12}\\n\",\n    \"\\\\end{equation}\\n\",\n    \"$$\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Based on the linearity of our model we previously found  ([5](#eq:analytic_mean_std)) which also yields:\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<!-- Equation labels as ordinary links -->\\n\",\n    \"<div id=\\\"eq:Ss_model_ded\\\"></div>\\n\",\n    \"\\n\",\n    \"$$\\n\",\n    \"\\\\begin{equation}\\n\",\n    \" \\\\sigma_Y^2 = \\\\sum_{i=1}^{r} \\\\left(\\\\Omega_i^2 \\\\, \\\\sigma_{Z_i}\\\\right)^2\\n\",\n    \"\\\\label{eq:Ss_model_ded} \\\\tag{13}\\n\",\n    \"\\\\end{equation}\\n\",\n    \"$$\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"As both ([13](#eq:Ss_model_ded)) and ([12](#eq:Ss_simple)) must hold simultaneously we get\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<!-- Equation labels as ordinary links -->\\n\",\n    \"<div id=\\\"eq:Ss1\\\"></div>\\n\",\n    \"\\n\",\n    \"$$\\n\",\n    \"\\\\begin{equation}\\n\",\n    \"\\\\left (S_{Z_i}^{\\\\sigma} \\\\right)^2=1 \\n\",\n    \"\\\\label{eq:Ss1} \\\\tag{14}\\n\",\n    \"\\\\end{equation}\\n\",\n    \"$$\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"The normalized derivative measure of sensitivity in ([11](#eq:Ss)) is\\n\",\n    \"more convincing than ([10](#eq:Sp)): first, as it involves both the\\n\",\n    \"weights $\\\\Omega_i$ and the factors $Z_i$ in ([2](#eq:linear_model));\\n\",\n    \"second as the measures are properly scaled and summarizes to one,\\n\",\n    \"which allows for an easy interpretation of the output sensitivity with\\n\",\n    \"respect to each of the input factors.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 8,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"    # Theoretical sensitivity indices\\n\",\n    \"    std_y = np.sqrt(np.sum((w * zm[:, 1])**2))\\n\",\n    \"    s = w * zm[:,1]/std_y\\n\",\n    \"    \\n\",\n    \"    print(\\\"\\\\nTheoretical sensitivity indices\\\\n\\\")\\n\",\n    \"    row_labels= ['S_'+str(idx) for idx in range(1,Nrv+1)]\\n\",\n    \"    print(pd.DataFrame(s**2, columns=['S analytic'],index=row_labels).round(3))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Based on samples of the random input variables and \\n\",\n    \"subsequent model evaluations, we may estimate the standard deviation\\n\",\n    \"of $\\\\mathbf{Y}$ and compute the relative error with respect to the\\n\",\n    \"theoretical value. You may change the number of sample above,\\n\",\n    \"i.e. $N$, and see how $N$ influence the estimates.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 9,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"    #  Expectation and variance from sampled values\\n\",\n    \"    \\n\",\n    \"    print(\\\"Expectation and std from sampled values\\\\n\\\")\\n\",\n    \"    print('std(Y)={:2.3f} and relative error={:2.3f}'.format(np.std(Y, 0), (np.std(Y, 0) - std_y) / std_y))\\n\",\n    \"    print('mean(Y)={:2.3f} and E(Y)={:2.3}'.format(np.mean(Y, 0), np.sum(zm[:,0]*w)))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Note that `Ns` is the size of our Monte Carlo experiment, corresponding\\n\",\n    \"to the number of times we have evaluated our simple linear model\\n\",\n    \"([2](#eq:linear_model)). The evaluation of the model is normally the\\n\",\n    \"most computationally expensive part of the analysis, and for that\\n\",\n    \"reasons `Ns` is referred to as the `cost` of the analysis.\\n\",\n    \"\\n\",\n    \"# Conditional variances\\n\",\n    \"\\n\",\n    \"As noted previously, the importance of a factor $Z_i$ is manifested\\n\",\n    \"the existence of a `shape` or `pattern` in the model outputs\\n\",\n    \"$Y$. Conversely, a uniform cloud of output points $Y$ as a function of\\n\",\n    \"$Z_i$ is a symptom, albeit not a proof, indicating that $Z_i$ is a\\n\",\n    \"noninfluential factor. In this section we seek to demonstrate that\\n\",\n    \"conditional variances is a usefull means to quantify the `shape` or\\n\",\n    \"`pattern` in the outputs.\\n\",\n    \"\\n\",\n    \"The shape in the outputs $Y$ for a given $Z_i$, may be seen in the\\n\",\n    \"scatterplot as of $Y$ versus $Z_i$. In particular, we may cut the\\n\",\n    \"$Z_i$-axis into slices and assess how the distribution of the outputs\\n\",\n    \"$Y$ changes from slice to slice. This is illustrated in the code\\n\",\n    \"snippet below, where the slices are identified with vertical dashed\\n\",\n    \"lines at equidistant locations on each $Z_i$-axis, $i=1, \\\\ldots,4$.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 10,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"    # # Scatter plots of data, z-slices, and linear model\\n\",\n    \"    fig=plt.figure()\\n\",\n    \"\\n\",\n    \"    Ndz = 10  # Number of slices of the Z-axes\\n\",\n    \"\\n\",\n    \"    Zslice = np.zeros((Nrv, Ndz))  # array for mean-values in the slices\\n\",\n    \"    ZBndry = np.zeros((Nrv, Ndz + 1))  # array for boundaries of the slices\\n\",\n    \"    dz = np.zeros(Nrv)\\n\",\n    \"\\n\",\n    \"    for k in range(Nrv):\\n\",\n    \"        plt.subplot(2, 2, k + 1)\\n\",\n    \"\\n\",\n    \"        zmin = np.min(Z[k, :])\\n\",\n    \"        zmax = np.max(Z[k, :])  # each Z[k,:] may have different extremas\\n\",\n    \"        dz[k] = (zmax - zmin) / Ndz\\n\",\n    \"\\n\",\n    \"        ZBndry[k, :] = np.linspace(zmin, zmax, Ndz + 1) # slice Zk into Ndz slices\\n\",\n    \"        Zslice[k, :] = np.linspace(zmin + dz[k] / 2., zmax - dz[k] / 2., Ndz) # Midpoint in the slice\\n\",\n    \"\\n\",\n    \"        # Plot the the vertical slices with axvline\\n\",\n    \"        for i in range(Ndz):\\n\",\n    \"            plt.axvline(ZBndry[k, i], np.amin(Y), np.amax(Y), linestyle='--', color='.75')\\n\",\n    \"\\n\",\n    \"        # Plot the data\\n\",\n    \"        plt.plot(Z[k, :], Y[:], '.')\\n\",\n    \"        xlbl = 'Z' + str(k)\\n\",\n    \"        plt.xlabel(xlbl)\\n\",\n    \"        plt.ylabel('Y')\\n\",\n    \"\\n\",\n    \"        Ymodel = w[k] * Zslice[k, :]  # Produce the straight line\\n\",\n    \"\\n\",\n    \"        plt.plot(Zslice[k, :], Ymodel)\\n\",\n    \"\\n\",\n    \"        ymin = np.amin(Y); ymax = np.amax(Y)\\n\",\n    \"        plt.ylim([ymin, ymax])\\n\",\n    \"    \\n\",\n    \"    fig.tight_layout()  # adjust subplot(s) to the figure area.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Note, that average value of $Y$ in a very thin slice, corresponds to\\n\",\n    \"keeping $Z_i$ fixed while averaging over all output values of $Y$ due\\n\",\n    \"to all-but $Z_i$, which corresponds to the conditional expected value:\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<!-- Equation labels as ordinary links -->\\n\",\n    \"<div id=\\\"_auto4\\\"></div>\\n\",\n    \"\\n\",\n    \"$$\\n\",\n    \"\\\\begin{equation}\\n\",\n    \"E_{Z_{\\\\sim i}} (Y\\\\;|\\\\;Z_i) \\n\",\n    \"\\\\label{_auto4} \\\\tag{15}\\n\",\n    \"\\\\end{equation}\\n\",\n    \"$$\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"For convenience we let $Z_{\\\\sim i}$ denote `all-but` $Z_i$. Naturally,\\n\",\n    \"a measure of how much $E_{Z_{\\\\sim i}} (Y\\\\;|\\\\;Z_i)$ varies in the range\\n\",\n    \"of $Z_i$ is given by the conditional variance:\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<!-- Equation labels as ordinary links -->\\n\",\n    \"<div id=\\\"_auto5\\\"></div>\\n\",\n    \"\\n\",\n    \"$$\\n\",\n    \"\\\\begin{equation}\\n\",\n    \"\\\\text{V}_{Z_i}(E_{Z_{\\\\sim i}} (Y\\\\;|\\\\;Z_i))\\n\",\n    \"\\\\label{_auto5} \\\\tag{16}\\n\",\n    \"\\\\end{equation}\\n\",\n    \"$$\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Further, the variance the output $Y$ may be decomposed into:\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<!-- Equation labels as ordinary links -->\\n\",\n    \"<div id=\\\"eq:VarDecomp\\\"></div>\\n\",\n    \"\\n\",\n    \"$$\\n\",\n    \"\\\\begin{equation}\\n\",\n    \"\\\\text{V}(Y) = E_{Z_i} ( V_{Z_{\\\\sim i}} (Y \\\\; | Z_{i})) + \\\\text{V}_{Z_i}(E_{Z_{\\\\sim i}} (Y\\\\;|\\\\;Z_i))\\n\",\n    \"\\\\label{eq:VarDecomp} \\\\tag{17}\\n\",\n    \"\\\\end{equation}\\n\",\n    \"$$\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"A large $\\\\text{V}_{Z_i}(E_{Z_{\\\\sim i}} (Y\\\\;|\\\\;Z_i))$ will imply that\\n\",\n    \"$Z_i$ is an important factor and is therefore coined the first-order\\n\",\n    \"effect of $Z_i$ on $Y$, and its fraction of the total variation of $Y$ is expressed by $S_i$, `the first-order sensitivity index` of $Z_i$ on $Y$:\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<!-- Equation labels as ordinary links -->\\n\",\n    \"<div id=\\\"_auto6\\\"></div>\\n\",\n    \"\\n\",\n    \"$$\\n\",\n    \"\\\\begin{equation}\\n\",\n    \"S_i = \\\\frac{\\\\text{V}_{Z_i}(E_{Z_{\\\\sim i}} (Y\\\\;|\\\\;Z_i))}{\\\\text{V}(Y)}\\n\",\n    \"\\\\label{_auto6} \\\\tag{18}\\n\",\n    \"\\\\end{equation}\\n\",\n    \"$$\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"By ([17](#eq:VarDecomp)), $S_i$ is number always in the range $[0,1]$,\\n\",\n    \"and a high value implies an important factor.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 11,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"    # # Scatter plots of averaged y-values per slice, with averaged data\\n\",\n    \"\\n\",\n    \"    Zsorted = np.zeros_like(Z)\\n\",\n    \"    Ysorted = np.zeros_like(Z)\\n\",\n    \"    YsliceMean = np.zeros((Nrv, Ndz))\\n\",\n    \"\\n\",\n    \"    fig=plt.figure()\\n\",\n    \"    for k in range(Nrv):\\n\",\n    \"        plt.subplot(2, 2, k + 1)\\n\",\n    \"\\n\",\n    \"        # sort values for Zk, \\n\",\n    \"        sidx = np.argsort(Z[k, :]) #sidx holds the indexes for the sorted values of Zk\\n\",\n    \"        Zsorted[k, :] = Z[k, sidx].copy()\\n\",\n    \"        Ysorted[k, :] = Y[sidx].copy()  # Ysorted is Y for the sorted Zk\\n\",\n    \"\\n\",\n    \"        for i in range(Ndz):\\n\",\n    \"            plt.axvline(ZBndry[k, i], np.amin(Y), np.amax(Y), linestyle='--', color='.75')\\n\",\n    \"\\n\",\n    \"            # find indexes of z-values in the current slice\\n\",\n    \"            zidx_range = np.logical_and(Zsorted[k, :] >= ZBndry[k, i], Zsorted[k, :] < ZBndry[k, i + 1])\\n\",\n    \"\\n\",\n    \"            if np.any(zidx_range):  # check if range has elements\\n\",\n    \"                YsliceMean[k, i] = np.mean(Ysorted[k, zidx_range])\\n\",\n    \"            else:  # set value to None if noe elements in z-slice\\n\",\n    \"                YsliceMean[k, i] = None\\n\",\n    \"\\n\",\n    \"        plt.plot(Zslice[k, :], YsliceMean[k, :], '.')\\n\",\n    \"        \\n\",\n    \"        \\n\",\n    \"\\n\",\n    \"        # # Plot linear model\\n\",\n    \"        Nmodel = 3\\n\",\n    \"        zmin = np.min(Zslice[k, :])\\n\",\n    \"        zmax = np.max(Zslice[k, :])\\n\",\n    \"\\n\",\n    \"        zvals = np.linspace(zmin, zmax, Nmodel)\\n\",\n    \"        #linear_model\\n\",\n    \"        Ymodel = w[k] * zvals\\n\",\n    \"        plt.plot(zvals, Ymodel)\\n\",\n    \"\\n\",\n    \"        xlbl = 'Z' + str(k)\\n\",\n    \"        plt.xlabel(xlbl)\\n\",\n    \"\\n\",\n    \"        plt.ylim(ymin, ymax)\\n\",\n    \"    \\n\",\n    \"    fig.tight_layout()  # adjust subplot(s) to the figure area.\\n\",\n    \"    \\n\",\n    \"    SpoorMan=[np.nanvar(YsliceMean[k,:],axis=0)/np.var(Y) for k in range(4)]   \\n\",\n    \"    print(SpoorMan)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# How to compute the sensitivity indices\\n\",\n    \"\\n\",\n    \"Below we will demostrate how the Sobol sensitivity indices may be\\n\",\n    \"computed with two approaches; the Monte Carlo method and the\\n\",\n    \"polynomial chaos expansion method.\\n\",\n    \"\\n\",\n    \"### Monte Carlo\\n\",\n    \"\\n\",\n    \"Below some code snippets are provided to illustrate how we may compute\\n\",\n    \"the Soboil indices with the MCM. For the interested reader we have also\\n\",\n    \"writen a seperate and more detailed notebook [A brief introduction to\\n\",\n    \"UQ and SA with the Monte Carlo method](monte_carlo.ipynb).\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 12,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"# calculate sens indices of non additive model\\n\",\n    \"def mc_sensitivity_linear(Ns, jpdf, w, sample_method='R'):\\n\",\n    \"\\n\",\n    \"    Nrv = len(jpdf)\\n\",\n    \"\\n\",\n    \"    # 1. Generate sample matrices\\n\",\n    \"    A, B, C = generate_sample_matrices_mc(Ns, Nrv, jpdf, sample_method)\\n\",\n    \"\\n\",\n    \"    # 2. Evaluate the model\\n\",\n    \"    Y_A, Y_B, Y_C = evaluate_linear_model(A, B, C, w)\\n\",\n    \"\\n\",\n    \"    # 3. Approximate the sensitivity indices\\n\",\n    \"    S, ST = calculate_sensitivity_indices_mc(Y_A, Y_B, Y_C)\\n\",\n    \"\\n\",\n    \"    return A, B, C, Y_A, Y_B, Y_C, S, ST\\n\",\n    \"# end calculate sens indices of non additive model\\n\",\n    \"\\n\",\n    \"\\n\",\n    \"# model evaluation\\n\",\n    \"def evaluate_linear_model(A, B, C, w):\\n\",\n    \"\\n\",\n    \"    number_of_parameters = A.shape[1]\\n\",\n    \"    number_of_sampless = A.shape[0]\\n\",\n    \"    # 1. evaluate sample matrices A\\n\",\n    \"    Y_A = linear_model(w, A)\\n\",\n    \"\\n\",\n    \"    # 2. evaluate sample matrices B\\n\",\n    \"    Y_B = linear_model(w, B)\\n\",\n    \"\\n\",\n    \"    # 3. evaluate sample matrices C\\n\",\n    \"    Y_C = np.empty((number_of_sampless, number_of_parameters))\\n\",\n    \"    for i in range(number_of_parameters):\\n\",\n    \"        z = C[i, :, :]\\n\",\n    \"        Y_C[:, i] = linear_model(w, z)\\n\",\n    \"\\n\",\n    \"    return Y_A, Y_B, Y_C\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 13,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"    # Monte Carlo\\n\",\n    \"    # get joint distributions\\n\",\n    \"    jpdf = generate_distributions(zm)\\n\",\n    \"\\n\",\n    \"    Ns_mc = 1000000\\n\",\n    \"    # calculate sensitivity indices\\n\",\n    \"    A_s, B_s, C_s, f_A, f_B, f_C, S_mc, ST_mc = mc_sensitivity_linear(Ns_mc, jpdf, w)\\n\",\n    \"\\n\",\n    \"    Sensitivities=np.column_stack((S_mc,s**2))\\n\",\n    \"    row_labels= ['S_'+str(idx) for idx in range(1,Nrv+1)]\\n\",\n    \"    print(\\\"First Order Indices\\\")\\n\",\n    \"    print(pd.DataFrame(Sensitivities,columns=['Smc','Sa'],index=row_labels).round(3))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Polynomial chaos expansion\\n\",\n    \"\\n\",\n    \"As for the MCM some code snippets are provided to illustrate how we may compute\\n\",\n    \"the Soboil indices with the polynomial chaos expansions using `chaospy`. A more in dept treatment of `chaospy` and its usage is provided in the separate notebook [A practical introduction to polynomial chaos with the chaospy package](introduction_gpc.ipynb).\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 14,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"    # Polychaos computations\\n\",\n    \"    Ns_pc = 80\\n\",\n    \"    samples_pc = jpdf.sample(Ns_pc)\\n\",\n    \"    polynomial_order = 4\\n\",\n    \"    poly = cp.orth_ttr(polynomial_order, jpdf)\\n\",\n    \"    Y_pc = linear_model(w, samples_pc.T)\\n\",\n    \"    approx = cp.fit_regression(poly, samples_pc, Y_pc, rule=\\\"T\\\")\\n\",\n    \"\\n\",\n    \"    exp_pc = cp.E(approx, jpdf)\\n\",\n    \"    std_pc = cp.Std(approx, jpdf)\\n\",\n    \"    print(\\\"Statistics polynomial chaos\\\\n\\\")\\n\",\n    \"    print('\\\\n        E(Y)  |  std(Y) \\\\n')\\n\",\n    \"    print('pc  : {:2.5f} | {:2.5f}'.format(float(exp_pc), std_pc))\\n\",\n    \"    \\n\",\n    \"    \\n\",\n    \"    S_pc = cp.Sens_m(approx, jpdf)\\n\",\n    \"\\n\",\n    \"    Sensitivities=np.column_stack((S_mc,S_pc, s**2))\\n\",\n    \"    print(\\\"\\\\nFirst Order Indices\\\")\\n\",\n    \"    print(pd.DataFrame(Sensitivities,columns=['Smc','Spc','Sa'],index=row_labels).round(3))\\n\",\n    \"\\n\",\n    \"#     print(\\\"\\\\nRelative errors\\\")\\n\",\n    \"#     rel_errors=np.column_stack(((S_mc - s**2)/s**2,(S_pc - s**2)/s**2))\\n\",\n    \"#     print(pd.DataFrame(rel_errors,columns=['Error Smc','Error Spc'],index=row_labels).round(3))\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 15,\n   \"metadata\": {\n    \"collapsed\": false\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"    # Polychaos convergence\\n\",\n    \"    Npc_list = np.logspace(1, 3, 10).astype(int)\\n\",\n    \"    error = []\\n\",\n    \"\\n\",\n    \"    for i, Npc in enumerate(Npc_list):\\n\",\n    \"        Zpc = jpdf.sample(Npc)\\n\",\n    \"        Ypc = linear_model(w, Zpc.T)\\n\",\n    \"        Npol = 4\\n\",\n    \"        poly = cp.orth_chol(Npol, jpdf)\\n\",\n    \"        approx = cp.fit_regression(poly, Zpc, Ypc, rule=\\\"T\\\")\\n\",\n    \"        s_pc = cp.Sens_m(approx, jpdf)\\n\",\n    \"        error.append(LA.norm((s_pc - s**2)/s**2))\\n\",\n    \"\\n\",\n    \"    plt.figure()\\n\",\n    \"    plt.semilogy(Npc_list, error)\\n\",\n    \"    _=plt.xlabel('Nr Z')\\n\",\n    \"    _=plt.ylabel('L2-norm of error in Sobol indices')\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# References\\n\",\n    \"\\n\",\n    \" 1. <div id=\\\"saltelli_global_2008\\\"></div> **A. Saltelli**. \\n\",\n    \"    *Global Sensitivity Analysis : the Primer*,\\n\",\n    \"    John Wiley,\\n\",\n    \"    2008.\"\n   ]\n  }\n ],\n \"metadata\": {},\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n"}}},"pageContext":{"permalink":"/posts/ipynb-test2/"}}